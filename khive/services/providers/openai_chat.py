from enum import Enum
from typing import Any, Dict, List, Literal, Optional, Union

from pydantic import BaseModel, Field, RootModel

from khive.services.endpoint import Endpoint, EndpointConfig

__all__ = (
    "OpenaiChatEndpoint",
    "OpenrouterChatEndpoint",
)


class ChatRole(str, Enum):
    """Enumeration for possible roles in a chat message."""

    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"


class ResponseFormatType(str, Enum):
    """Enumeration for response format types."""

    TEXT = "text"
    JSON_OBJECT = "json_object"


class ToolType(str, Enum):
    """Enumeration for tool types."""

    FUNCTION = "function"


# --- Nested Models ---


class FunctionParameters(RootModel[Dict[str, Any]]):
    """
    Represents the parameters the functions accepts, described as a JSON Schema object.
    See OpenAI's guide for examples and the JSON Schema reference for documentation.
    Ref: https://platform.openai.com/docs/guides/function-calling
    Ref: https://json-schema.org/understanding-json-schema/
    """

    root: Dict[str, Any] = Field(
        ...,
        description="The parameters the functions accepts, described as a JSON Schema object.",
    )


class FunctionDefinition(BaseModel):
    """Defines a function tool."""

    name: str = Field(..., description="The name of the function to be called.")
    description: Optional[str] = Field(
        None, description="A description of what the function does."
    )
    parameters: Optional[FunctionParameters] = Field(
        None,
        description="The parameters the function accepts, as a JSON Schema object.",
    )


class ToolDefinition(BaseModel):
    """Defines a tool available to the model."""

    type: Literal[ToolType.FUNCTION] = Field(
        ..., description="The type of the tool, currently only 'function'."
    )
    function: FunctionDefinition = Field(..., description="The function definition.")


class FunctionCall(BaseModel):
    """Represents a function call requested by the model."""

    name: str = Field(..., description="The name of the function to call.")
    arguments: str = Field(
        ..., description="The arguments to call the function with, as a JSON string."
    )


class ToolCall(BaseModel):
    """Represents a tool call requested by the model."""

    id: str = Field(..., description="The ID of the tool call.")
    type: Literal[ToolType.FUNCTION] = Field(
        ..., description="The type of the tool called, e.g., 'function'."
    )
    function: FunctionCall = Field(
        ..., description="The function that the model invoked."
    )


class ChatMessage(BaseModel):
    """Represents a single message in the chat history."""

    role: ChatRole = Field(..., description="The role of the author of this message.")
    content: Optional[str] = Field(None, description="The contents of the message.")
    tool_calls: Optional[List[ToolCall]] = Field(
        None,
        description="The tool calls generated by the model, if any. Role must be 'assistant'.",
    )
    tool_call_id: Optional[str] = Field(
        None,
        description="Tool call that this message is responding to. Role must be 'tool'.",
    )


class ResponseFormat(BaseModel):
    """Specifies the format the model must output."""

    type: ResponseFormatType = Field(
        ..., description="The type of response format. Use 'json_object' for JSON mode."
    )


# --- Main Request Model ---


class ChatCompletionRequest(BaseModel):
    """
    Pydantic model for the OpenAI Chat Completion API request body.
    Endpoint: POST https://api.openai.com/v1/chat/completions
    Docs: https://platform.openai.com/docs/api-reference/chat/create
    """

    messages: List[ChatMessage] = Field(
        ..., description="A list of messages describing the conversation so far."
    )
    model: str = Field(
        ...,
        description="ID of the model to use (e.g., 'gpt-4o', 'gpt-4-turbo', 'gpt-3.5-turbo'). See model endpoint compatibility: https://platform.openai.com/docs/models",
    )

    # Optional parameters
    frequency_penalty: Optional[float] = Field(
        None,
        ge=-2.0,
        le=2.0,
        description="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
    )
    logit_bias: Optional[Dict[str, int]] = Field(
        None,
        description="Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object mapping token IDs (int) to bias values (-100 to 100).",
    )
    logprobs: Optional[bool] = Field(
        False, description="Whether to return log probabilities of the output tokens."
    )
    top_logprobs: Optional[int] = Field(
        None,
        ge=0,
        le=20,
        description="An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with log probability. Requires `logprobs` to be True.",
    )
    max_tokens: Optional[int] = Field(
        None,
        description="The maximum number of tokens to generate in the chat completion.",
    )
    n: Optional[int] = Field(
        1,
        ge=1,
        description="How many chat completion choices to generate for each input message.",
    )
    presence_penalty: Optional[float] = Field(
        None,
        ge=-2.0,
        le=2.0,
        description="Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
    )
    response_format: Optional[ResponseFormat] = Field(
        None,
        description="An object specifying the format that the model must output. Setting to { 'type': 'json_object' } enables JSON mode.",
    )
    seed: Optional[int] = Field(
        None,
        description="If specified, the system will make a best effort to sample deterministically.",
    )
    stop: Optional[Union[str, List[str]]] = Field(
        None,
        description="Up to 4 sequences where the API will stop generating further tokens.",
    )
    stream: Optional[bool] = Field(
        False,
        description="If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available.",
    )
    temperature: Optional[float] = Field(
        1.0,
        ge=0.0,
        le=2.0,
        description="Sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
    )
    top_p: Optional[float] = Field(
        1.0,
        ge=0.0,
        le=1.0,
        description="Nucleus sampling parameter. The model considers the results of the tokens with top_p probability mass. So 0.1 means only tokens comprising the top 10% probability mass are considered. Alter temperature or top_p, but not both.",
    )
    tools: Optional[List[ToolDefinition]] = Field(
        None, description="A list of tools the model may call."
    )
    tool_choice: Optional[Union[Literal["none", "auto"], Dict[str, Any]]] = Field(
        None,
        description="Controls which (if any) function is called by the model. 'none' means no function call, 'auto' is default. Can specify a specific function like {'type': 'function', 'function': {'name': 'my_function'}}.",
    )
    user: Optional[str] = Field(
        None,
        description="A unique identifier representing your end-user, which can help OpenAI monitor and detect abuse.",
    )


OPENAI_CHAT_ENDPOINT_CONFIG = EndpointConfig(
    name="openai_chat",
    provider="openai",
    base_url=None,
    endpoint="chat/completions",
    kwargs={"model": "gpt-4o"},
    openai_compatible=True,
    api_key="OPENAI_API_KEY",
    auth_template={"Authorization": "Bearer $API_KEY"},
    default_headers={"content-type": "application/json"},
    request_options=ChatCompletionRequest,
)

OPENROUTER_CHAT_ENDPOINT_CONFIG = EndpointConfig(
    name="openrouter_chat",
    provider="openrouter",
    base_url="https://openrouter.ai/api/v1",
    endpoint="chat/completions",
    kwargs={"model": "gpt-4o"},
    openai_compatible=True,
    api_key="ollama_key",
    auth_template={"Authorization": "Bearer $API_KEY"},
    default_headers={"content-type": "application/json"},
    request_options=ChatCompletionRequest,
)


class OpenaiChatEndpoint(Endpoint):
    def __init__(self, config=OPENAI_CHAT_ENDPOINT_CONFIG, **kwargs):
        super().__init__(config, **kwargs)


class OpenrouterChatEndpoint(Endpoint):
    def __init__(self, config=OPENROUTER_CHAT_ENDPOINT_CONFIG, **kwargs):
        super().__init__(config, **kwargs)
