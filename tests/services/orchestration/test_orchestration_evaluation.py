"""Comprehensive test suite for orchestration evaluation system.

This module provides tests for:
- Decision matrix validation and complexity assessment
- Confidence scoring accuracy
- Pydantic model constraint testing
- External model integration mocking
- Performance and reliability testing
"""

import asyncio
import time
from unittest.mock import Mock, patch

import pytest
from pydantic import ValidationError

from khive.services.plan.cost_tracker import CostTracker
from khive.services.plan.models import OrchestrationEvaluation
from khive.services.plan.parts import (
    AgentRecommendation,
    ComplexityLevel,
    PlannerRequest,
    PlannerResponse,
    QualityGate,
    TaskPhase,
    WorkflowPattern,
)
from khive.services.plan.planner_service import (
    ComplexityTier,
    OrchestrationPlanner,
    PlannerService,
    Request,
)
from tests.fixtures.planning_fixtures import (
    MockOpenAIResponse,
)


@pytest.mark.unit
class TestDecisionMatrixValidation:
    """Test decision matrix loading and validation."""

    def test_valid_decision_matrix_structure(self, mock_decision_matrix):
        """Test that valid decision matrix loads correctly."""
        # Test required sections exist
        required_sections = ["complexity_assessment", "agent_role_selection"]
        for section in required_sections:
            assert section in mock_decision_matrix.data

        # Test complexity tiers are properly defined
        complexity_tiers = ["simple", "medium", "complex", "very_complex"]
        assessment = mock_decision_matrix.get("complexity_assessment", {})
        for tier in complexity_tiers:
            assert tier in assessment
            assert "indicators" in assessment[tier]
            assert isinstance(assessment[tier]["indicators"], list)

    def test_complexity_indicators_validation(self, mock_decision_matrix):
        """Test complexity indicators are properly structured."""
        assessment = mock_decision_matrix.get("complexity_assessment", {})

        for config in assessment.values():
            # Each tier must have indicators
            assert "indicators" in config
            assert len(config["indicators"]) > 0

            # Indicators should be strings
            for indicator in config["indicators"]:
                assert isinstance(indicator, str)
                assert len(indicator) > 0

    def test_agent_role_selection_validation(self, mock_decision_matrix):
        """Test agent role selection rules are valid."""
        role_selection = mock_decision_matrix.get("agent_role_selection", {})

        required_phases = [
            "discovery_phase",
            "design_phase",
            "implementation_phase",
            "validation_phase",
        ]
        for phase in required_phases:
            assert phase in role_selection
            assert "roles" in role_selection[phase]
            assert len(role_selection[phase]["roles"]) > 0

    def test_ragrs_triggers_validation(self, mock_decision_matrix):
        """Test RAGRS domain triggers are properly structured."""
        triggers = mock_decision_matrix.get("ragrs_domain_triggers", {})

        for config in triggers.values():
            assert "keywords" in config
            assert "mandatory_roles" in config
            assert isinstance(config["keywords"], list)
            assert isinstance(config["mandatory_roles"], list)

            # Ensure keywords and roles are non-empty strings
            for keyword in config["keywords"]:
                assert isinstance(keyword, str)
                assert len(keyword) > 0
            for role in config["mandatory_roles"]:
                assert isinstance(role, str)
                assert len(role) > 0

    def test_invalid_decision_matrix_handling(self):
        """Test handling of invalid decision matrix configurations."""
        # OrchestrationPlanner is robust and handles missing sections gracefully
        # This test verifies that the planner doesn't crash with incomplete matrices
        with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
            with patch(
                "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
            ) as mock_load:
                # Missing complexity_assessment section
                invalid_matrix = {"agent_role_selection": {}}
                mock_load.return_value = invalid_matrix
                
                planner = OrchestrationPlanner()
                planner.matrix = invalid_matrix
                # Should not crash, may return default values
                result = planner.assess(Request("test"))
                assert result is not None

            with patch(
                "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
            ) as mock_load:
                # Missing agent_role_selection section  
                invalid_matrix = {"complexity_assessment": {}}
                mock_load.return_value = invalid_matrix
                
                planner = OrchestrationPlanner()
                planner.matrix = invalid_matrix
                # Should not crash, may return empty list
                result = planner.select_roles(Request("test"), ComplexityTier.SIMPLE)
                assert isinstance(result, list)


@pytest.mark.unit
class TestComplexityAssessment:
    """Test complexity assessment algorithms."""

    def test_complexity_tier_matching(self, mock_decision_matrix, complexity_scenarios):
        """Test complexity tier assignment based on request patterns."""
        # Mock OrchestrationPlanner with decision matrix
        with patch(
            "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
        ) as mock_load:
            mock_load.return_value = mock_decision_matrix.data

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner = OrchestrationPlanner()
                planner.matrix = mock_decision_matrix.data

                for request_text, expected_complexity in complexity_scenarios:
                    request = Request(request_text)
                    assessed_complexity = planner.assess(request)

                    assert assessed_complexity == expected_complexity, (
                        f"Failed for '{request_text}': expected {expected_complexity}, "
                        f"got {assessed_complexity}"
                    )

    def test_heuristic_fallback(self, mock_decision_matrix):
        """Test heuristic-based complexity assessment when no direct indicators match."""
        with patch(
            "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
        ) as mock_load:
            mock_load.return_value = mock_decision_matrix.data

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner = OrchestrationPlanner()
                planner.matrix = mock_decision_matrix.data

                # Test requests without direct indicators
                test_cases = [
                    ("build api without specific keywords", ComplexityTier.MEDIUM),
                    ("simple quick easy basic task", ComplexityTier.SIMPLE),
                    (
                        "complex sophisticated advanced comprehensive system",
                        ComplexityTier.COMPLEX,
                    ),
                    (
                        "research novel cutting-edge entire platform",
                        ComplexityTier.VERY_COMPLEX,
                    ),
                ]

                for request_text, expected_complexity in test_cases:
                    request = Request(request_text)
                    assessed_complexity = planner.assess(request)
                    assert assessed_complexity == expected_complexity

    def test_ragrs_complexity_modifiers(self, mock_decision_matrix):
        """Test RAGRS-specific complexity modifiers."""
        with patch(
            "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
        ) as mock_load:
            mock_load.return_value = mock_decision_matrix.data

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner = OrchestrationPlanner()
                planner.matrix = mock_decision_matrix.data

                # Test distributed consensus modifier (should increase complexity by 1 level)
                request = Request("implement consensus with byzantine fault tolerance")
                assessed_complexity = planner.assess(request)

                # Should be escalated from complex to very_complex due to distributed_consensus modifier
                assert assessed_complexity in [
                    ComplexityTier.COMPLEX,
                    ComplexityTier.VERY_COMPLEX,
                ]

    def test_edge_case_requests(self, mock_decision_matrix, edge_case_requests):
        """Test complexity assessment with edge case inputs."""
        with patch(
            "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
        ) as mock_load:
            mock_load.return_value = mock_decision_matrix.data

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner = OrchestrationPlanner()
                planner.matrix = mock_decision_matrix.data

                for request_text, expected in edge_case_requests:
                    request = Request(request_text)
                    assessed_complexity = planner.assess(request)

                    # Should handle edge cases gracefully without crashing
                    assert isinstance(assessed_complexity, ComplexityTier)

                    if "expected_complexity" in expected:
                        assert assessed_complexity == expected["expected_complexity"]


@pytest.mark.unit
class TestRoleSelection:
    """Test agent role selection algorithms."""

    def test_role_selection_by_complexity(
        self, mock_decision_matrix, role_selection_scenarios
    ):
        """Test role selection varies appropriately by complexity level."""
        with patch(
            "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
        ) as mock_load:
            mock_load.return_value = mock_decision_matrix.data

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner = OrchestrationPlanner()
                planner.matrix = mock_decision_matrix.data

                for (
                    request_text,
                    complexity,
                    expected_roles_subset,
                ) in role_selection_scenarios:
                    request = Request(request_text)
                    selected_roles = planner.select_roles(request, complexity)

                    # Check that expected roles are included
                    for role in expected_roles_subset:
                        assert role in selected_roles, (
                            f"Expected role '{role}' not found in {selected_roles} "
                            f"for {complexity} complexity"
                        )

                    # Verify role count scales with complexity
                    if complexity == ComplexityTier.SIMPLE:
                        assert len(selected_roles) <= 4
                    elif complexity == ComplexityTier.VERY_COMPLEX:
                        assert len(selected_roles) >= 6

    def test_ragrs_mandatory_roles(self, mock_decision_matrix, ragrs_trigger_scenarios):
        """Test RAGRS mandatory role injection."""
        with patch(
            "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
        ) as mock_load:
            mock_load.return_value = mock_decision_matrix.data

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner = OrchestrationPlanner()
                planner.matrix = mock_decision_matrix.data

                for scenario in ragrs_trigger_scenarios:
                    request = Request(scenario["request"])
                    complexity = ComplexityTier.MEDIUM
                    selected_roles = planner.select_roles(request, complexity)

                    # Check mandatory roles are included
                    for role in scenario["expected_roles"]:
                        assert (
                            role in selected_roles
                        ), f"Mandatory role '{role}' missing for request: {scenario['request']}"

    def test_phase_dependency_mapping(self, mock_decision_matrix):
        """Test that role selection considers phase dependencies."""
        with patch(
            "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
        ) as mock_load:
            mock_load.return_value = mock_decision_matrix.data

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner = OrchestrationPlanner()
                planner.matrix = mock_decision_matrix.data

                # Test different request types to trigger different phases
                test_cases = [
                    ("research new algorithms", ["discovery_phase"]),
                    (
                        "design and build system",
                        ["design_phase", "implementation_phase"],
                    ),
                    ("test and validate solution", ["validation_phase"]),
                    (
                        "implement, test, and document",
                        [
                            "implementation_phase",
                            "validation_phase",
                            "refinement_phase",
                        ],
                    ),
                ]

                for request_text, expected_phases in test_cases:
                    request = Request(request_text)
                    planner._determine_required_phases(request)
                    # This would be tested by examining the internal phase determination


@pytest.mark.unit
class TestPydanticModelValidation:
    """Test Pydantic model validation and constraints."""

    def test_orchestration_evaluation_validation(self, sample_evaluation):
        """Test OrchestrationEvaluation model validation."""
        # Test valid model
        eval_dict = sample_evaluation.model_dump()
        validated = OrchestrationEvaluation.model_validate(eval_dict)
        assert validated == sample_evaluation

    def test_orchestration_evaluation_constraints(self):
        """Test OrchestrationEvaluation field constraints."""
        # Test complexity field constraints
        with pytest.raises(ValidationError):
            OrchestrationEvaluation(
                complexity="invalid_complexity",  # Should be one of the allowed literals
                complexity_reason="Test",
                total_agents=5,
                agent_reason="Test",
                rounds_needed=2,
                role_priorities=["researcher"],
                primary_domains=["test-domain"],
                domain_reason="Test",
                workflow_pattern="parallel",
                workflow_reason="Test",
                quality_level="thorough",
                quality_reason="Test",
                rules_applied=["test"],
                confidence=0.8,
                summary="Test",
            )

        # Test total_agents bounds
        with pytest.raises(ValidationError):
            OrchestrationEvaluation(
                complexity="medium",
                complexity_reason="Test",
                total_agents=0,  # Should be >= 1
                agent_reason="Test",
                rounds_needed=2,
                role_priorities=["researcher"],
                primary_domains=["test-domain"],
                domain_reason="Test",
                workflow_pattern="parallel",
                workflow_reason="Test",
                quality_level="thorough",
                quality_reason="Test",
                rules_applied=["test"],
                confidence=0.8,
                summary="Test",
            )

        with pytest.raises(ValidationError):
            OrchestrationEvaluation(
                complexity="medium",
                complexity_reason="Test",
                total_agents=25,  # Should be <= 20
                agent_reason="Test",
                rounds_needed=2,
                role_priorities=["researcher"],
                primary_domains=["test-domain"],
                domain_reason="Test",
                workflow_pattern="parallel",
                workflow_reason="Test",
                quality_level="thorough",
                quality_reason="Test",
                rules_applied=["test"],
                confidence=0.8,
                summary="Test",
            )

    def test_planner_request_validation(self):
        """Test PlannerRequest model validation."""
        # Valid request
        request = PlannerRequest(
            task_description="Test task",
            context="Test context",
            time_budget_seconds=60.0,
        )
        assert request.task_description == "Test task"

        # Test field requirements
        with pytest.raises(ValidationError):
            PlannerRequest()  # Missing required task_description

    def test_agent_recommendation_validation(self):
        """Test AgentRecommendation model validation."""
        # Valid recommendation
        rec = AgentRecommendation(
            role="researcher",
            domain="distributed-systems",
            priority=0.8,
            reasoning="Test reasoning",
        )
        assert rec.role == "researcher"

        # Test all fields required
        with pytest.raises(ValidationError):
            AgentRecommendation(role="researcher")  # Missing required fields

    def test_task_phase_validation(self, agent_recommendations):
        """Test TaskPhase model validation."""
        # Valid phase
        phase = TaskPhase(
            name="test_phase",
            description="Test phase",
            agents=agent_recommendations,
            quality_gate=QualityGate.THOROUGH,
            coordination_pattern=WorkflowPattern.PARALLEL,
        )
        assert phase.name == "test_phase"

        # Test enum validation
        with pytest.raises(ValidationError):
            TaskPhase(
                name="test_phase",
                description="Test phase",
                agents=agent_recommendations,
                quality_gate="invalid_gate",  # Invalid enum value
                coordination_pattern=WorkflowPattern.PARALLEL,
            )

    def test_planner_response_validation(self, task_phases):
        """Test PlannerResponse model validation."""
        # Valid response
        response = PlannerResponse(
            success=True,
            summary="Test summary",
            complexity=ComplexityLevel.MEDIUM,
            recommended_agents=5,
            phases=task_phases,
            confidence=0.85,
        )
        assert response.success is True

        # Test confidence bounds
        with pytest.raises(ValidationError):
            PlannerResponse(
                success=True,
                summary="Test summary",
                complexity=ComplexityLevel.MEDIUM,
                recommended_agents=5,
                confidence=1.5,  # Should be <= 1.0
            )


@pytest.mark.unit
class TestExternalModelMocking:
    """Test mocking framework for external planning models."""

    @pytest.fixture
    def mock_openai_environment(self):
        """Mock OpenAI environment for testing."""
        with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key_123"}):
            with patch(
                "khive.services.plan.planner_service.OpenAI"
            ) as mock_openai_class:
                mock_client = Mock()
                mock_openai_class.return_value = mock_client
                yield mock_client

    def test_openai_client_mocking(self, mock_openai_environment, sample_evaluation):
        """Test OpenAI client mocking for external API calls."""
        # Configure mock response
        mock_response = MockOpenAIResponse(sample_evaluation)
        mock_openai_environment.beta.chat.completions.parse.return_value = mock_response

        # Test that mocked client returns expected response
        response = mock_openai_environment.beta.chat.completions.parse(
            model="gpt-5-nano",
            messages=[{"role": "user", "content": "test"}],
            response_format=OrchestrationEvaluation,
        )

        assert response.choices[0].message.parsed == sample_evaluation
        assert response.usage.prompt_tokens == 100
        assert response.usage.completion_tokens == 200

    @pytest.mark.asyncio
    async def test_concurrent_evaluation_mocking(
        self, mock_openai_environment, sample_evaluation
    ):
        """Test mocking of concurrent evaluations."""
        # Mock multiple different responses
        responses = [
            MockOpenAIResponse(sample_evaluation),
            MockOpenAIResponse(sample_evaluation),
            MockOpenAIResponse(sample_evaluation),
        ]

        mock_openai_environment.beta.chat.completions.parse.side_effect = responses

        # Simulate concurrent evaluation calls
        tasks = []
        for i in range(3):

            async def mock_eval(index=i):
                await asyncio.sleep(0.01)  # Simulate API delay
                return mock_openai_environment.beta.chat.completions.parse(
                    model="gpt-5-nano",
                    messages=[{"role": "user", "content": f"test {index}"}],
                    response_format=OrchestrationEvaluation,
                )

            tasks.append(mock_eval())

        results = await asyncio.gather(*tasks)

        assert len(results) == 3
        for result in results:
            assert result.choices[0].message.parsed == sample_evaluation

    def test_api_error_simulation(self, mock_openai_environment):
        """Test simulation of API errors for resilience testing."""
        # Configure mock to raise various exceptions
        error_scenarios = [
            ConnectionError("Failed to connect"),
            TimeoutError("Request timed out"),
            ValueError("Invalid response format"),
        ]

        for error in error_scenarios:
            mock_openai_environment.beta.chat.completions.parse.side_effect = error

            with pytest.raises(type(error)):
                mock_openai_environment.beta.chat.completions.parse(
                    model="gpt-5-nano",
                    messages=[{"role": "user", "content": "test"}],
                    response_format=OrchestrationEvaluation,
                )

    def test_cost_tracking_integration(self, mock_cost_tracker):
        """Test cost tracking integration with mocked external calls."""
        # Test cost tracking methods
        cost = mock_cost_tracker.add_request(500, 200, 0)
        assert cost == 0.001

        # Test budget methods
        assert mock_cost_tracker.get_token_budget() == 10000
        assert mock_cost_tracker.get_latency_budget() == 5000
        assert mock_cost_tracker.get_cost_budget() == 1.0


@pytest.mark.unit
class TestConfidenceScoring:
    """Test confidence scoring accuracy validation."""

    def test_confidence_score_ranges(self, mock_decision_matrix):
        """Test that confidence scores fall within expected ranges."""
        with patch(
            "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
        ) as mock_load:
            mock_load.return_value = mock_decision_matrix.data

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner = OrchestrationPlanner()
                planner.matrix = mock_decision_matrix.data

                test_cases = [
                    (
                        "simple well defined single objective task",
                        (0.7, 1.0),
                    ),  # High confidence
                    (
                        "complex distributed system with some unknowns",
                        (0.5, 0.8),
                    ),  # Medium confidence
                    ("vague comprehensive solution", (0.3, 0.6)),  # Low confidence
                ]

                for request_text, (min_conf, max_conf) in test_cases:
                    request = Request(request_text)
                    complexity = planner.assess(request)

                    # Confidence would be determined by the evaluation process
                    # For unit tests, we check the assessment logic works
                    assert isinstance(complexity, ComplexityTier)

    def test_evaluator_agreement_variance(self):
        """Test that different evaluators show reasonable variance."""
        # This would test the consensus building mechanism
        # where multiple evaluators provide different assessments

    def test_confidence_calibration(self):
        """Test confidence score calibration against known outcomes."""
        # This would test confidence scores against historical accuracy


@pytest.mark.integration
class TestEndToEndOrchestration:
    """Integration tests for complete orchestration flow."""

    @pytest.mark.asyncio
    async def test_simple_task_orchestration(self, integration_test_scenarios):
        """Test end-to-end orchestration for simple tasks."""
        simple_scenario = next(
            s
            for s in integration_test_scenarios
            if s["name"] == "simple_task_end_to_end"
        )

        with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
            with patch(
                "khive.services.plan.planner_service.OrchestrationPlanner.evaluate_request"
            ) as mock_eval:
                # Mock evaluation response
                mock_eval.return_value = [
                    {
                        "config": {"name": "test_evaluator"},
                        "evaluation": OrchestrationEvaluation(
                            complexity="simple",
                            complexity_reason="Basic CRUD operations",
                            total_agents=2,
                            agent_reason="Minimal team needed",
                            rounds_needed=1,
                            role_priorities=["implementer", "tester"],
                            primary_domains=["backend-development"],
                            domain_reason="API development focus",
                            workflow_pattern="sequential",
                            workflow_reason="Simple pipeline",
                            quality_level="basic",
                            quality_reason="Straightforward requirements",
                            rules_applied=["complexity_assessment"],
                            confidence=0.9,
                            summary="Simple API development task",
                        ),
                        "cost": 0.001,
                        "usage": Mock(prompt_tokens=100, completion_tokens=50),
                        "response_time_ms": 500,
                    }
                ]

                planner_service = PlannerService()
                response = await planner_service.handle_request(
                    simple_scenario["request"]
                )

                assert response.success is True
                assert response.complexity == simple_scenario["expected_complexity"]
                assert (
                    simple_scenario["expected_agent_range"][0]
                    <= response.recommended_agents
                    <= simple_scenario["expected_agent_range"][1]
                )
                assert len(response.phases) == simple_scenario["expected_phases"]

    @pytest.mark.asyncio
    async def test_complex_distributed_system_orchestration(
        self, integration_test_scenarios
    ):
        """Test end-to-end orchestration for complex distributed systems."""
        complex_scenario = next(
            s
            for s in integration_test_scenarios
            if s["name"] == "complex_distributed_system"
        )

        with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
            with patch(
                "khive.services.plan.planner_service.OrchestrationPlanner.evaluate_request"
            ) as mock_eval:
                # Mock complex evaluation response
                mock_eval.return_value = [
                    {
                        "config": {"name": "complex_evaluator"},
                        "evaluation": OrchestrationEvaluation(
                            complexity="very_complex",
                            complexity_reason="Byzantine fault tolerance requires formal verification",
                            total_agents=12,
                            agent_reason="Comprehensive expertise needed across multiple domains",
                            rounds_needed=4,
                            role_priorities=[
                                "theorist",
                                "researcher",
                                "architect",
                                "critic",
                                "implementer",
                                "tester",
                                "auditor",
                            ],
                            primary_domains=[
                                "byzantine-fault-tolerance",
                                "distributed-consensus",
                            ],
                            domain_reason="Core distributed systems and fault tolerance expertise",
                            workflow_pattern="hybrid",
                            workflow_reason="Parallel research with sequential validation",
                            quality_level="critical",
                            quality_reason="Safety-critical distributed system",
                            rules_applied=["ragrs_triggers", "complexity_modifiers"],
                            confidence=0.85,
                            summary="Complex distributed consensus requiring formal verification",
                        ),
                        "cost": 0.003,
                        "usage": Mock(prompt_tokens=800, completion_tokens=400),
                        "response_time_ms": 2000,
                    }
                ]

                planner_service = PlannerService()
                response = await planner_service.handle_request(
                    complex_scenario["request"]
                )

                assert response.success is True
                assert response.complexity == complex_scenario["expected_complexity"]
                assert (
                    complex_scenario["expected_agent_range"][0]
                    <= response.recommended_agents
                    <= complex_scenario["expected_agent_range"][1]
                )
                assert len(response.phases) == complex_scenario["expected_phases"]

                # Validate specific criteria for complex systems
                criteria = complex_scenario["validation_criteria"]
                phase_names = [phase.name for phase in response.phases]
                all_roles = []
                for phase in response.phases:
                    all_roles.extend([agent.role for agent in phase.agents])

                if criteria.get("has_theorist"):
                    assert "theorist" in all_roles
                if criteria.get("has_critic"):
                    assert "critic" in all_roles
                if criteria.get("has_auditor"):
                    assert "auditor" in all_roles
                if criteria.get("includes_validation_phase"):
                    assert any("validation" in name for name in phase_names)


@pytest.mark.performance
class TestPerformanceAndReliability:
    """Performance and reliability tests for orchestration system."""

    @pytest.mark.asyncio
    async def test_concurrent_evaluation_performance(self, performance_test_config):
        """Test performance under concurrent evaluation load."""
        concurrent_count = performance_test_config["concurrent_evaluations"]
        max_time = performance_test_config["max_response_time_ms"] / 1000.0

        with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
            with patch(
                "khive.services.plan.planner_service.OrchestrationPlanner.evaluate_request"
            ) as mock_eval:
                # Mock fast response
                mock_eval.return_value = [
                    {
                        "evaluation": OrchestrationEvaluation(
                            complexity="medium",
                            complexity_reason="Test",
                            total_agents=5,
                            agent_reason="Test",
                            rounds_needed=2,
                            role_priorities=["researcher", "implementer"],
                            primary_domains=["software-architecture"],
                            domain_reason="Test",
                            workflow_pattern="parallel",
                            workflow_reason="Test",
                            quality_level="thorough",
                            quality_reason="Test",
                            rules_applied=["test"],
                            confidence=0.8,
                            summary="Test evaluation",
                        ),
                        "cost": 0.001,
                        "usage": Mock(prompt_tokens=100, completion_tokens=50),
                        "response_time_ms": 200,
                    }
                ]

                # Create concurrent planning requests
                planner_service = PlannerService()
                tasks = []

                start_time = time.time()
                for i in range(concurrent_count):
                    request = PlannerRequest(
                        task_description=f"Test task {i}",
                        context="Performance test",
                    )
                    task = planner_service.handle_request(request)
                    tasks.append(task)

                # Execute concurrently
                responses = await asyncio.gather(*tasks)
                end_time = time.time()

                # Validate performance
                total_time = end_time - start_time
                assert (
                    total_time < max_time
                ), f"Performance test failed: {total_time}s > {max_time}s"

                # Validate all responses succeeded
                for response in responses:
                    assert response.success is True

    def test_memory_usage_limits(self, performance_test_config):
        """Test memory usage stays within limits."""
        memory_limit = performance_test_config["memory_limit_mb"]

        # This would test memory usage during large orchestration planning
        # Could use memory profiling tools like memory_profiler

    @pytest.mark.asyncio
    async def test_timeout_handling(self, performance_test_config):
        """Test graceful handling of various timeout scenarios."""
        timeout_scenarios = performance_test_config["timeout_scenarios"]

        for scenario in timeout_scenarios:
            timeout = scenario["timeout"]
            expected_behavior = scenario["expected_behavior"]

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner_service = PlannerService()
                request = PlannerRequest(
                    task_description="Test task with timeout",
                    time_budget_seconds=timeout,
                )

                # Test timeout behavior based on expected outcome
                if expected_behavior == "graceful_degradation":
                    # Should return partial results quickly
                    start_time = time.time()
                    response = await planner_service.handle_request(request)
                    elapsed = time.time() - start_time

                    assert elapsed <= timeout * 1.2  # Allow 20% tolerance
                    # Should still provide some result even if degraded
                    assert response.success is True or response.summary != ""

                elif expected_behavior == "full_completion":
                    # Should complete normally within timeout
                    response = await planner_service.handle_request(request)
                    assert response.success is True

    def test_cost_budget_compliance(self, performance_test_config):
        """Test compliance with cost budgets."""
        target_cost = performance_test_config["target_cost_per_evaluation"]

        # Mock cost tracking
        cost_tracker = CostTracker()
        cost_tracker.set_cost_budget(target_cost)

        # Test that cost tracking works
        cost = cost_tracker.add_request(500, 200, 0)
        assert cost <= target_cost * 1.5  # Allow reasonable tolerance

        # Test budget checking
        if cost > target_cost:
            assert cost_tracker.is_over_budget()


@pytest.mark.regression
class TestRegressionSafeguards:
    """Regression tests to prevent known issues from reoccurring."""

    def test_empty_role_list_handling(self, mock_decision_matrix):
        """Regression test for empty role list handling."""
        with patch(
            "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
        ) as mock_load:
            mock_load.return_value = mock_decision_matrix.data

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner = OrchestrationPlanner()
                planner.matrix = mock_decision_matrix.data

                # Test with request that might produce empty role list
                request = Request("")
                complexity = ComplexityTier.SIMPLE
                roles = planner.select_roles(request, complexity)

                # Should never return empty role list
                assert len(roles) > 0
                assert "implementer" in roles or "researcher" in roles

    def test_circular_dependency_prevention(self):
        """Regression test for circular dependency detection."""
        # Test that dependency resolution doesn't create infinite loops

    def test_unicode_request_handling(self, mock_decision_matrix):
        """Regression test for Unicode character handling in requests."""
        with patch(
            "khive.services.plan.planner_service.OrchestrationPlanner._load_decision_matrix"
        ) as mock_load:
            mock_load.return_value = mock_decision_matrix.data

            with patch.dict("os.environ", {"OPENAI_API_KEY": "test_key"}):
                planner = OrchestrationPlanner()
                planner.matrix = mock_decision_matrix.data

                # Test with Unicode characters
                unicode_requests = [
                    "è®¾è®¡åˆ†å¸ƒå¼ç³»ç»Ÿ",  # Chinese
                    "diseÃ±ar sistema distribuido",  # Spanish with accents
                    "ðŸ”¬ research with emojis ðŸ§ª",  # Emojis
                    "API fÃ¼r groÃŸe Datenmengen",  # German with umlauts
                ]

                for request_text in unicode_requests:
                    request = Request(request_text)
                    complexity = planner.assess(request)

                    # Should handle Unicode gracefully
                    assert isinstance(complexity, ComplexityTier)

                    roles = planner.select_roles(request, complexity)
                    assert len(roles) > 0
